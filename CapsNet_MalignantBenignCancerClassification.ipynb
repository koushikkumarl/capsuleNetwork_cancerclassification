{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:78: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:83: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import numpy as np\n",
    "import os, math, csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras import initializers, layers, optimizers\n",
    "from keras.layers import (Input,Conv2D,Activation,Dense,Flatten,Reshape,Dropout)\n",
    "from keras.layers.merge import add\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report, r2_score\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickleImages(object):\n",
    "\n",
    "    def __init__(self,PATH='', IMAGE_SIZE = 28): #Default image size set to 28 px\n",
    "        self.PATH = PATH\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "        self.image_data = []\n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        self.CATEGORIES = []\n",
    "        self.list_categories = [] \n",
    "\n",
    "    def get_categories(self):\n",
    "        for path in os.listdir(self.PATH):\n",
    "            if '.DS_Store' in path:\n",
    "                pass\n",
    "            else:\n",
    "                self.list_categories.append(path)\n",
    "        print(\"Found Categories \",self.list_categories,'\\n')\n",
    "        return self.list_categories\n",
    "\n",
    "    def Process_Image(self):\n",
    "        try:\n",
    "            self.CATEGORIES = self.get_categories()\n",
    "            for categories in self.CATEGORIES:                                                  \n",
    "                train_folder_path = os.path.join(self.PATH, categories)                         \n",
    "                class_index = self.CATEGORIES.index(categories)                                 \n",
    "                for img in os.listdir(train_folder_path):                                       \n",
    "                    new_path = os.path.join(train_folder_path, img)                             \n",
    "                    try:\n",
    "                        image_data_temp = cv2.imread(new_path)                 # Read Image as numbers\n",
    "                        image_temp_resize = cv2.resize(image_data_temp,(self.IMAGE_SIZE,self.IMAGE_SIZE))\n",
    "                        self.image_data.append([image_temp_resize,class_index])\n",
    "                    except:\n",
    "                        pass\n",
    "            data = np.asanyarray(self.image_data)\n",
    "\n",
    "            # Iterate over the Data\n",
    "            for x in data:\n",
    "                self.x_data.append(x[0])        \n",
    "                self.y_data.append(x[1])        \n",
    "            X_Data = np.asarray(self.x_data)    \n",
    "            Y_Data = np.asarray(self.y_data)\n",
    "            # reshape x_Data as required! \n",
    "            #X_Data = X_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 1)\n",
    "            #Y_Data = Y_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 1)\n",
    "            return X_Data, Y_Data\n",
    "        except:\n",
    "            print(\"Failed to run Function Process Image \")\n",
    "\n",
    "    def pickle_image(self):\n",
    "        # Call the Function and Get the Data\n",
    "        X_Data,Y_Data = self.Process_Image()\n",
    "        # Write the Entire Data into a Pickle File\n",
    "        pickle_out = open('X_Data','wb')\n",
    "        pickle.dump(X_Data, pickle_out)\n",
    "        pickle_out.close()\n",
    "        # Write the Y Label Data\n",
    "        pickle_out = open('Y_Data', 'wb')\n",
    "        pickle.dump(Y_Data, pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "        print(\"Pickled Image Successfully \")\n",
    "        return X_Data,Y_Data\n",
    "    \n",
    "    \n",
    "    #If pickled imageset is available, rest of process doesn't get initiated! \n",
    "    def load_dataset(self):\n",
    "        try:\n",
    "            # Read the Data from Pickle Object\n",
    "            X_Temp = open('X_Data','rb')\n",
    "            X_Data = pickle.load(X_Temp)\n",
    "            Y_Temp = open('Y_Data','rb')\n",
    "            Y_Data = pickle.load(Y_Temp)\n",
    "            print('Reading Dataset from Pickle Object')\n",
    "            return X_Data,Y_Data\n",
    "\n",
    "        except:\n",
    "            print('Could not Found Pickle File ')\n",
    "            print('Loading File and Dataset  ..........')\n",
    "            X_Data,Y_Data = self.pickle_image()\n",
    "            return X_Data,Y_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf, group_names=None, categories='auto', count=True, percent=True, cbar=True, xyticks=True,\n",
    "                          xyplotlabels=True, sum_stats=False, figsize=None, cmap='Blues', title=None):\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Dataset from Pickle Object\n"
     ]
    }
   ],
   "source": [
    "path = './ColonLungCancer' #Provide image path here!\n",
    "a = PickleImages(PATH=path, IMAGE_SIZE=64) #Provide required image size\n",
    "X_Data,Y_Data = a.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 64, 64, 3)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_Data.shape)\n",
    "print(Y_Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.6\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "num_classes = 5\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_Data, Y_Data, test_size=1 - train_ratio, random_state = 123)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_train /= 255.0 \n",
    "x_test /= 255.0  \n",
    "x_val /= 255.0 \n",
    "\n",
    "y_true = y_test\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "y_val = to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 64, 64, 3)\n",
      "(5000, 64, 64, 3)\n",
      "(5000, 64, 64, 3)\n",
      "(15000, 5)\n",
      "(5000, 5)\n",
      "(5000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer():\n",
    "    if not os.path.exists('results/'):\n",
    "        os.mkdir('results')\n",
    "    if not os.path.exists('weights/'):\n",
    "        os.mkdir('weights')\n",
    "\n",
    "def data_generator(x,y,batch_size):\n",
    "    x_train,y_train = x,y\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    generator = datagen.flow(x_train,y_train,batch_size=batch_size,shuffle=True)\n",
    "    while True:\n",
    "        x,y  = generator.next()\n",
    "        yield ([x,y],[y,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "        Mask Tensor layer by the max value in first axis\n",
    "        Input shape: [None,d1,d2]\n",
    "        Output shape: [None,d2]\n",
    "    \"\"\"\n",
    "    clip_value = (0,1)\n",
    "\n",
    "    def Mask(self,clip_value=(0,1),**kwargs):\n",
    "        self.clip_value = clip_value # what if clip value is not 0 and 1?\n",
    "\n",
    "    def call(self,inputs,**kwargs):\n",
    "        if type(inputs) is list:\n",
    "            assert len(inputs) == 2\n",
    "            inputs,mask = inputs\n",
    "        else:\n",
    "            x = inputs\n",
    "            # enlarge range of values in x by mapping max(new_x) = 1, others \n",
    "            x = (x - K.max(x,1,True)) / K.epsilon() + 1\n",
    "            mask = K.clip(x,self.clip_value[0],self.clip_value[1]) # clip value beween 0 and 1\n",
    "        masked_input = K.batch_dot(inputs, mask, [1,1])\n",
    "        return masked_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:\n",
    "            return tuple([None,input_shape[0][-1]])\n",
    "    \n",
    "        else:\n",
    "            return tuple([None, input_shape[-1]])\n",
    "        \n",
    "def squash(vector, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vector), axis, keepdims=True)\n",
    "    scale = s_squared_norm/(1+s_squared_norm)/K.sqrt(s_squared_norm)\n",
    "    return scale*vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        assert len(input_shape) >= 3, \"Input tensor must have shape=[None, input_num_capsule,input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsule,self.num_capsule,self.input_dim_vector,self.dim_vector],\n",
    "                                initializer=self.kernel_initializer,\n",
    "                                name='W')\n",
    "        self.bias = self.add_weight(shape=[1,self.input_num_capsule,self.num_capsule,1,1],\n",
    "                                initializer=self.bias_initializer,\n",
    "                                name='bias',trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self,inputs,training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\"  \n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        \n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        \"\"\"\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCapsule(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
    "    return layers.Lambda(squash)(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(input,kernel_size=8,filters=16,kernel_regularizer=l2(1.e-4)):\n",
    "    conv2 = Conv2D(filters=filters,kernel_size=kernel_size,kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_initializer=\"he_normal\",padding=\"same\")(input)\n",
    "    norm = BatchNormalization(axis=3)(conv2)\n",
    "    activation = Activation(\"relu\")(norm)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3590: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1188: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-11-0f0898886771>:59: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "x_train shape: (15000, 64, 64, 3)\n",
      "15000 train samples\n",
      "5000 test samples\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 64, 64, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 64, 64, 32)    24608       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 64, 64, 32)    128         conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 64, 64, 32)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 28, 28, 72)    186696      activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 6272, 9)       0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 6272, 9)       0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "routing_layer_1 (CapsuleLayer)   (None, 5, 18)         5111680     lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 5)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "mask_1 (Mask)                    (None, 18)            0           routing_layer_1[0][0]            \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 16)            304         mask_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 32)            544         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 12288)         405504      dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Length)                  (None, 5)             0           routing_layer_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "output_recon (Reshape)           (None, 64, 64, 3)     0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 5,729,464\n",
      "Trainable params: 5,698,040\n",
      "Non-trainable params: 31,424\n",
      "____________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=234, validation_data=([array([[..., epochs=30, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:899: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:886: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Koushik\\.conda\\envs\\cifardataset\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.3338 - output_loss: 0.3149 - output_recon_loss: 0.1389 - output_acc: 0.5985Epoch 00000: val_loss improved from inf to 0.98032, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 198s - loss: 0.3332 - output_loss: 0.3143 - output_recon_loss: 0.1384 - output_acc: 0.5989 - val_loss: 0.9803 - val_output_loss: 0.9700 - val_output_recon_loss: 0.0530 - val_output_acc: 0.1966\n",
      "Epoch 2/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.1717 - output_loss: 0.1635 - output_recon_loss: 0.0328 - output_acc: 0.7788Epoch 00001: val_loss improved from 0.98032 to 0.48423, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 198s - loss: 0.1716 - output_loss: 0.1634 - output_recon_loss: 0.0327 - output_acc: 0.7791 - val_loss: 0.4842 - val_output_loss: 0.4757 - val_output_recon_loss: 0.0363 - val_output_acc: 0.4474\n",
      "Epoch 3/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.1473 - output_loss: 0.1393 - output_recon_loss: 0.0316 - output_acc: 0.8134Epoch 00002: val_loss improved from 0.48423 to 0.24011, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 196s - loss: 0.1472 - output_loss: 0.1392 - output_recon_loss: 0.0316 - output_acc: 0.8136 - val_loss: 0.2401 - val_output_loss: 0.2321 - val_output_recon_loss: 0.0320 - val_output_acc: 0.7282\n",
      "Epoch 4/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.1328 - output_loss: 0.1249 - output_recon_loss: 0.0314 - output_acc: 0.8330Epoch 00003: val_loss did not improve\n",
      "234/234 [==============================] - 201s - loss: 0.1327 - output_loss: 0.1248 - output_recon_loss: 0.0314 - output_acc: 0.8333 - val_loss: 0.3534 - val_output_loss: 0.3455 - val_output_recon_loss: 0.0311 - val_output_acc: 0.7594\n",
      "Epoch 5/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.1222 - output_loss: 0.1144 - output_recon_loss: 0.0311 - output_acc: 0.8444Epoch 00004: val_loss improved from 0.24011 to 0.19423, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 199s - loss: 0.1223 - output_loss: 0.1145 - output_recon_loss: 0.0311 - output_acc: 0.8441 - val_loss: 0.1942 - val_output_loss: 0.1864 - val_output_recon_loss: 0.0315 - val_output_acc: 0.7708\n",
      "Epoch 6/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.1112 - output_loss: 0.1035 - output_recon_loss: 0.0310 - output_acc: 0.8603Epoch 00005: val_loss did not improve\n",
      "234/234 [==============================] - 201s - loss: 0.1115 - output_loss: 0.1038 - output_recon_loss: 0.0310 - output_acc: 0.8595 - val_loss: 0.2512 - val_output_loss: 0.2433 - val_output_recon_loss: 0.0325 - val_output_acc: 0.6984\n",
      "Epoch 7/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.1062 - output_loss: 0.0986 - output_recon_loss: 0.0308 - output_acc: 0.8680Epoch 00006: val_loss did not improve\n",
      "234/234 [==============================] - 197s - loss: 0.1063 - output_loss: 0.0986 - output_recon_loss: 0.0308 - output_acc: 0.8678 - val_loss: 0.2102 - val_output_loss: 0.2025 - val_output_recon_loss: 0.0321 - val_output_acc: 0.7496\n",
      "Epoch 8/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0956 - output_loss: 0.0881 - output_recon_loss: 0.0308 - output_acc: 0.8838Epoch 00007: val_loss improved from 0.19423 to 0.11744, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 199s - loss: 0.0956 - output_loss: 0.0880 - output_recon_loss: 0.0308 - output_acc: 0.8838 - val_loss: 0.1174 - val_output_loss: 0.1099 - val_output_recon_loss: 0.0309 - val_output_acc: 0.8580\n",
      "Epoch 9/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0922 - output_loss: 0.0846 - output_recon_loss: 0.0307 - output_acc: 0.8924Epoch 00008: val_loss did not improve\n",
      "234/234 [==============================] - 197s - loss: 0.0921 - output_loss: 0.0846 - output_recon_loss: 0.0307 - output_acc: 0.8925 - val_loss: 0.1446 - val_output_loss: 0.1371 - val_output_recon_loss: 0.0307 - val_output_acc: 0.8132\n",
      "Epoch 10/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0831 - output_loss: 0.0757 - output_recon_loss: 0.0306 - output_acc: 0.9084Epoch 00009: val_loss did not improve\n",
      "234/234 [==============================] - 199s - loss: 0.0831 - output_loss: 0.0756 - output_recon_loss: 0.0306 - output_acc: 0.9085 - val_loss: 0.3212 - val_output_loss: 0.3135 - val_output_recon_loss: 0.0329 - val_output_acc: 0.7348\n",
      "Epoch 11/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0767 - output_loss: 0.0693 - output_recon_loss: 0.0306 - output_acc: 0.9182Epoch 00010: val_loss improved from 0.11744 to 0.10640, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 199s - loss: 0.0766 - output_loss: 0.0692 - output_recon_loss: 0.0306 - output_acc: 0.9182 - val_loss: 0.1064 - val_output_loss: 0.0990 - val_output_recon_loss: 0.0309 - val_output_acc: 0.8580\n",
      "Epoch 12/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0687 - output_loss: 0.0614 - output_recon_loss: 0.0305 - output_acc: 0.9349Epoch 00011: val_loss did not improve\n",
      "234/234 [==============================] - 195s - loss: 0.0687 - output_loss: 0.0614 - output_recon_loss: 0.0305 - output_acc: 0.9349 - val_loss: 0.1411 - val_output_loss: 0.1337 - val_output_recon_loss: 0.0309 - val_output_acc: 0.8316\n",
      "Epoch 13/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0614 - output_loss: 0.0540 - output_recon_loss: 0.0305 - output_acc: 0.9486Epoch 00012: val_loss improved from 0.10640 to 0.08182, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 195s - loss: 0.0613 - output_loss: 0.0540 - output_recon_loss: 0.0305 - output_acc: 0.9488 - val_loss: 0.0818 - val_output_loss: 0.0745 - val_output_recon_loss: 0.0307 - val_output_acc: 0.9146\n",
      "Epoch 14/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0599 - output_loss: 0.0526 - output_recon_loss: 0.0304 - output_acc: 0.9500Epoch 00013: val_loss did not improve\n",
      "234/234 [==============================] - 197s - loss: 0.0599 - output_loss: 0.0526 - output_recon_loss: 0.0304 - output_acc: 0.9501 - val_loss: 0.0918 - val_output_loss: 0.0845 - val_output_recon_loss: 0.0305 - val_output_acc: 0.9058\n",
      "Epoch 15/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0530 - output_loss: 0.0458 - output_recon_loss: 0.0304 - output_acc: 0.9625Epoch 00014: val_loss did not improve\n",
      "234/234 [==============================] - 199s - loss: 0.0530 - output_loss: 0.0458 - output_recon_loss: 0.0304 - output_acc: 0.9625 - val_loss: 0.1792 - val_output_loss: 0.1719 - val_output_recon_loss: 0.0306 - val_output_acc: 0.8468\n",
      "Epoch 16/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0488 - output_loss: 0.0416 - output_recon_loss: 0.0304 - output_acc: 0.9670Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/234 [==============================] - 186s - loss: 0.0489 - output_loss: 0.0417 - output_recon_loss: 0.0304 - output_acc: 0.9670 - val_loss: 0.1356 - val_output_loss: 0.1284 - val_output_recon_loss: 0.0312 - val_output_acc: 0.8342\n",
      "Epoch 17/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0457 - output_loss: 0.0385 - output_recon_loss: 0.0304 - output_acc: 0.9715Epoch 00016: val_loss improved from 0.08182 to 0.08169, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 189s - loss: 0.0457 - output_loss: 0.0385 - output_recon_loss: 0.0304 - output_acc: 0.9716 - val_loss: 0.0817 - val_output_loss: 0.0745 - val_output_recon_loss: 0.0305 - val_output_acc: 0.9194\n",
      "Epoch 18/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0410 - output_loss: 0.0339 - output_recon_loss: 0.0303 - output_acc: 0.9779Epoch 00017: val_loss did not improve\n",
      "234/234 [==============================] - 197s - loss: 0.0410 - output_loss: 0.0339 - output_recon_loss: 0.0303 - output_acc: 0.9779 - val_loss: 0.0863 - val_output_loss: 0.0792 - val_output_recon_loss: 0.0304 - val_output_acc: 0.9090\n",
      "Epoch 19/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0372 - output_loss: 0.0301 - output_recon_loss: 0.0302 - output_acc: 0.9810Epoch 00018: val_loss did not improve\n",
      "234/234 [==============================] - 197s - loss: 0.0373 - output_loss: 0.0302 - output_recon_loss: 0.0302 - output_acc: 0.9810 - val_loss: 0.0885 - val_output_loss: 0.0814 - val_output_recon_loss: 0.0304 - val_output_acc: 0.8856\n",
      "Epoch 20/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0364 - output_loss: 0.0294 - output_recon_loss: 0.0303 - output_acc: 0.9820Epoch 00019: val_loss improved from 0.08169 to 0.07721, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 196s - loss: 0.0364 - output_loss: 0.0293 - output_recon_loss: 0.0303 - output_acc: 0.9821 - val_loss: 0.0772 - val_output_loss: 0.0701 - val_output_recon_loss: 0.0303 - val_output_acc: 0.9262\n",
      "Epoch 21/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0312 - output_loss: 0.0242 - output_recon_loss: 0.0303 - output_acc: 0.9869Epoch 00020: val_loss did not improve\n",
      "234/234 [==============================] - 191s - loss: 0.0313 - output_loss: 0.0242 - output_recon_loss: 0.0303 - output_acc: 0.9867 - val_loss: 0.0910 - val_output_loss: 0.0840 - val_output_recon_loss: 0.0305 - val_output_acc: 0.9072\n",
      "Epoch 22/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0301 - output_loss: 0.0231 - output_recon_loss: 0.0302 - output_acc: 0.9881Epoch 00021: val_loss improved from 0.07721 to 0.07548, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 179s - loss: 0.0301 - output_loss: 0.0231 - output_recon_loss: 0.0302 - output_acc: 0.9882 - val_loss: 0.0755 - val_output_loss: 0.0685 - val_output_recon_loss: 0.0303 - val_output_acc: 0.9182\n",
      "Epoch 23/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0276 - output_loss: 0.0206 - output_recon_loss: 0.0302 - output_acc: 0.9903Epoch 00022: val_loss improved from 0.07548 to 0.05762, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 182s - loss: 0.0276 - output_loss: 0.0206 - output_recon_loss: 0.0302 - output_acc: 0.9904 - val_loss: 0.0576 - val_output_loss: 0.0506 - val_output_recon_loss: 0.0302 - val_output_acc: 0.9512\n",
      "Epoch 24/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0262 - output_loss: 0.0192 - output_recon_loss: 0.0301 - output_acc: 0.9904Epoch 00023: val_loss did not improve\n",
      "234/234 [==============================] - 186s - loss: 0.0262 - output_loss: 0.0192 - output_recon_loss: 0.0302 - output_acc: 0.9905 - val_loss: 0.0589 - val_output_loss: 0.0519 - val_output_recon_loss: 0.0302 - val_output_acc: 0.9542\n",
      "Epoch 25/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0246 - output_loss: 0.0176 - output_recon_loss: 0.0302 - output_acc: 0.9932Epoch 00024: val_loss improved from 0.05762 to 0.05245, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 182s - loss: 0.0246 - output_loss: 0.0176 - output_recon_loss: 0.0302 - output_acc: 0.9931 - val_loss: 0.0525 - val_output_loss: 0.0455 - val_output_recon_loss: 0.0301 - val_output_acc: 0.9588\n",
      "Epoch 26/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0230 - output_loss: 0.0161 - output_recon_loss: 0.0301 - output_acc: 0.9932Epoch 00025: val_loss did not improve\n",
      "234/234 [==============================] - 181s - loss: 0.0231 - output_loss: 0.0161 - output_recon_loss: 0.0301 - output_acc: 0.9931 - val_loss: 0.0528 - val_output_loss: 0.0459 - val_output_recon_loss: 0.0301 - val_output_acc: 0.9598\n",
      "Epoch 27/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0222 - output_loss: 0.0153 - output_recon_loss: 0.0301 - output_acc: 0.9946Epoch 00026: val_loss improved from 0.05245 to 0.05062, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 185s - loss: 0.0222 - output_loss: 0.0153 - output_recon_loss: 0.0301 - output_acc: 0.9947 - val_loss: 0.0506 - val_output_loss: 0.0437 - val_output_recon_loss: 0.0301 - val_output_acc: 0.9608\n",
      "Epoch 28/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0208 - output_loss: 0.0139 - output_recon_loss: 0.0302 - output_acc: 0.9954Epoch 00027: val_loss improved from 0.05062 to 0.05009, saving model to weights/capsule-10Train_45Val_45Test5.h5\n",
      "234/234 [==============================] - 182s - loss: 0.0208 - output_loss: 0.0139 - output_recon_loss: 0.0302 - output_acc: 0.9955 - val_loss: 0.0501 - val_output_loss: 0.0432 - val_output_recon_loss: 0.0301 - val_output_acc: 0.9598\n",
      "Epoch 29/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0200 - output_loss: 0.0131 - output_recon_loss: 0.0301 - output_acc: 0.9959Epoch 00028: val_loss did not improve\n",
      "234/234 [==============================] - 183s - loss: 0.0200 - output_loss: 0.0131 - output_recon_loss: 0.0301 - output_acc: 0.9958 - val_loss: 0.0528 - val_output_loss: 0.0460 - val_output_recon_loss: 0.0302 - val_output_acc: 0.9594\n",
      "Epoch 30/30\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.0192 - output_loss: 0.0123 - output_recon_loss: 0.0301 - output_acc: 0.9956Epoch 00029: val_loss did not improve\n",
      "234/234 [==============================] - 175s - loss: 0.0192 - output_loss: 0.0123 - output_recon_loss: 0.0301 - output_acc: 0.9956 - val_loss: 0.0627 - val_output_loss: 0.0559 - val_output_recon_loss: 0.0301 - val_output_acc: 0.9408\n"
     ]
    }
   ],
   "source": [
    "def CapsNetv2(input_shape,n_class,n_route,n_prime_caps=8,dense_size = (16,32)):\n",
    "    conv_filter = 32\n",
    "    n_kernel = 8\n",
    "    primary_channel =8\n",
    "    primary_vector = 9\n",
    "    capsule_dim_size = 9\n",
    "\n",
    "    target_shape = input_shape\n",
    "\n",
    "    input = Input(shape=input_shape)\n",
    "    conv_block_1 = convolution_block(input,kernel_size=16,filters=32)\n",
    "    primary_cap = PrimaryCapsule(conv_block_1,dim_vector=capsule_dim_size,n_channels=primary_channel,kernel_size=9,strides=2,padding='valid')    \n",
    "    routing_layer = CapsuleLayer(num_capsule=n_class,dim_vector=capsule_dim_size*2,num_routing=n_route,name='routing_layer_1')(primary_cap)\n",
    "    output = Length(name='output')(routing_layer)\n",
    "    y = Input(shape=(n_class,))\n",
    "    masked = Mask()([routing_layer,y])\n",
    "    x_recon = Dense(dense_size[0],activation='relu')(masked)\n",
    "\n",
    "    for i in range(1,len(dense_size)):\n",
    "        x_recon = Dense(dense_size[i],activation='relu')(x_recon)\n",
    "    x_recon = Dense(np.prod(target_shape),activation='relu')(x_recon)\n",
    "    x_recon = Reshape(target_shape=target_shape,name='output_recon')(x_recon)\n",
    "    return Model([input,y],[output,x_recon])\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))\n",
    "\n",
    "def train(epochs=200,batch_size=64,mode=1):\n",
    "    num_classes = 5\n",
    "    model = CapsNetv2(input_shape=[64, 64, 3],\n",
    "                        n_class=num_classes,\n",
    "                        n_route=3)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    model.summary()\n",
    "    log = callbacks.CSVLogger('results/capsule-10Train_45Val_45Test'+str(num_classes)+'-log.csv')\n",
    "    tb = callbacks.TensorBoard(log_dir='results/tensorboard-capsule-cifar-'+str(num_classes)+'-logs',\n",
    "                               batch_size=batch_size, histogram_freq=True)\n",
    "    checkpoint = callbacks.ModelCheckpoint('weights/capsule-10Train_45Val_45Test'+str(num_classes)+'.h5',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "    #plot_model(model, to_file='models/capsule-cifar-'+str(num_classes)+'.png', show_shapes=True)\n",
    "\n",
    "    model.compile(optimizer=optimizers.adam(lr=0.1),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.1],\n",
    "                  metrics={'output':'accuracy'})\n",
    "    #from utils.helper_function import data_generator\n",
    "\n",
    "    generator = data_generator(x_train,y_train,batch_size)\n",
    "    # Image generator significantly increase the accuracy and reduce validation loss\n",
    "    model.fit_generator(generator,\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        validation_data=([x_val, y_val], [y_val, x_val]),\n",
    "                        epochs=epochs, verbose=1, max_q_size=100,\n",
    "                        callbacks=[log,checkpoint,lr_decay])\n",
    "\n",
    "\n",
    "train(30, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded, start validation\n",
      "--------------------------------------------------\n",
      "Test acc: 0.954\n"
     ]
    }
   ],
   "source": [
    "model = CapsNetv2(input_shape=[64, 64, 3], n_class=5, n_route=3)\n",
    "model.load_weights('weights/capsule-10Train_45Val_45Test5'+'.h5') \n",
    "print(\"Weights loaded, start validation\")   \n",
    "y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n",
    "print('-'*50)\n",
    "print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = (y_pred > 0.8) \n",
    "y_prediction = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[ 918   68    3    0    0]\n",
      " [  33  953    0    0    0]\n",
      " [  12    0  953    4   54]\n",
      " [   0    0    7 1006    0]\n",
      " [   1    0   48    0  940]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix: ') \n",
    "print(confusion_matrix(y_true, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.9541418846449847\n",
      "Recall score: 0.954\n",
      "F1 Score: 0.9539751872338488\n",
      "Accuracy Score: 0.954\n",
      "R2 score: 0.9398639120329305\n",
      "Classification Score: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       989\n",
      "           1       0.93      0.97      0.95       986\n",
      "           2       0.94      0.93      0.94      1023\n",
      "           3       1.00      0.99      0.99      1013\n",
      "           4       0.95      0.95      0.95       989\n",
      "\n",
      "    accuracy                           0.95      5000\n",
      "   macro avg       0.95      0.95      0.95      5000\n",
      "weighted avg       0.95      0.95      0.95      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Precision Score: ' + str(precision_score(y_true, y_prediction, average='weighted')))\n",
    "print('Recall score: ' + str(recall_score(y_true, y_prediction, average='weighted')))\n",
    "print('F1 Score: ' + str(f1_score(y_true, y_prediction, average='weighted')))\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_true, y_prediction, normalize=True, sample_weight=None)))\n",
    "print('R2 score: ' + str(r2_score(y_true, y_prediction,multioutput='variance_weighted')))\n",
    "print('Classification Score: \\n' + str(classification_report(y_true, y_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = ['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc']\n",
    "make_confusion_matrix(confusion_matrix(y_true, y_prediction), categories = categories,sum_stats=False, title='Colon - Lung Cancer - Confusion Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cifardataset",
   "language": "python",
   "name": "cifardataset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
